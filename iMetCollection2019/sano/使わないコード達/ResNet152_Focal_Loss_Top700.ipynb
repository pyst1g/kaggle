{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fl: flip <br>\n",
    "ro: rotation <br>\n",
    "no: normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "log_dir =  runs/ResNet152_Focal_Loss_Top700\n",
      "weight save path =  ./model_weight/resnet152/ResNet152_Focal_Loss_Top700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa53e9cf610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import models\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "use_model = 'resnet152'\n",
    "this_file = 'ResNet152_Focal_Loss_Top700'\n",
    "\n",
    "if use_model == '': use_model = 'a'\n",
    "if this_file == '': this_file = 'a'\n",
    "\n",
    "# data_path = \"~/Datasets/iMet_Colelction_2019\"\n",
    "# load_path = \"../input/\"\n",
    "load_path = \"/home/sano/Datasets/iMet_Colelction_2019/input/\"\n",
    "\n",
    "log_dir = 'runs/' + this_file\n",
    "weight_path = './model_weight/' + use_model + '/' + this_file\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)\n",
    "\n",
    "print('log_dir = ', log_dir)\n",
    "print('weight save path = ', weight_path)\n",
    "# 複数GPU使用宣言\n",
    "# if device == 'cuda:1':\n",
    "#     net = torch.nn.DataParallel(net) # make parallel\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "\n",
    "torch.manual_seed(823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 1103\n",
    "epochs = 50\n",
    "extract_attribute = 5 # 予測した上位何個を属性として出力するか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iMetsDataset(data.Dataset):\n",
    " \n",
    "    def __init__(self, df, root_dir, transform=None, mode='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (dataframe): ファイル名がindex、Nhot_LabelsカラムにNhot化したラベルを格納したDataframe\n",
    "            root_dir (string): 対象の画像ファイルが入っているフォルダ\n",
    "            transform (callable, optional): 施す変換\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "#         if type(idx) == torch.Tensor:\n",
    "#             idx = idx.item()\n",
    "        img_name = os.path.join(self.root_dir, self.df.index[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            label = self.df.iloc[idx].Nhot_Labels_TopN.astype('float32')\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "def Nhot_encoding(arr, l):\n",
    "    \"\"\"\n",
    "    Nhotエンコーディングを行う\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : ndarray\n",
    "        ラベル\n",
    "    l : int\n",
    "        総ラベル数\n",
    "    \"\"\"\n",
    "    if arr.ndim == 1:\n",
    "        ret = np.zeros(l,dtype='int')\n",
    "        if len(arr) != 0:\n",
    "            ret[arr] = 1\n",
    "        return ret\n",
    "    else:\n",
    "        lst = list()\n",
    "        for i,_ in enumerate(arr):\n",
    "            lst.extend([i] * arr.shape[1])\n",
    "            \n",
    "        ret = np.zeros((arr.shape[0],l),dtype='int')\n",
    "        ret[lst,arr.flatten()] = 1\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = pd.read_csv(load_path + 'labels.csv')\n",
    "label_name = label_name.set_index(\"attribute_id\")\n",
    "submit_df = pd.read_csv(load_path + 'sample_submission.csv')\n",
    "submit_df[\"id\"] = submit_df[\"id\"].apply(lambda x: x + \".png\")\n",
    "submit_df = submit_df.set_index('id')\n",
    "test_size = len(submit_df)\n",
    "\n",
    "train_df = pd.read_csv(load_path + 'train.csv')\n",
    "train_size = len(train_df)\n",
    "train_df[\"attribute_ids\"] = train_df[\"attribute_ids\"].apply(lambda x: np.array([int(s) for s in x.split(\" \")]))\n",
    "train_df[\"Nhot_Labels\"] = train_df[\"attribute_ids\"].apply(lambda x: Nhot_encoding(x,1103))\n",
    "train_df[\"id\"] = train_df[\"id\"].apply(lambda x: x + \".png\")\n",
    "train_df = train_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#上位何個取るか\n",
    "take_num = 700\n",
    "\n",
    "cnt_attribute = Counter()\n",
    "for i in train_df.attribute_ids:\n",
    "    cnt_attribute.update(i)\n",
    "\n",
    "freq_attr = np.asarray(cnt_attribute.most_common(num_classes))\n",
    "\n",
    "# plt.ylim(0,15000)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(1,len(freq_attr)+1),freq_attr[:,1],width=1)\n",
    "plt.title(\"frequency\")\n",
    "plt.hlines(freq_attr[take_num-1][1], 0, take_num, 'red', linestyles='dashed', linewidth=1)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(1,len(freq_attr)+1),freq_attr[:,1],width=1)\n",
    "plt.hlines(freq_attr[take_num-1][1], 0, take_num, 'red', linestyles='dashed', linewidth=1)\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.title('log-scale')\n",
    "\n",
    "freq_attr = np.asarray(cnt_attribute.most_common(take_num))\n",
    "freq_set = set(freq_attr[:take_num,0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sort_label_encode = {a:b for b,a in enumerate(freq_attr[:,0])}\n",
    "sort_label_decode = {a:b for a,b in enumerate(freq_attr[:,0])}\n",
    "\n",
    "train_df[\"attribute_ids_TopN\"] = train_df[\"attribute_ids\"].apply(lambda x:np.array([sort_label_encode[i] for i in x if i in freq_set]))\n",
    "train_df[\"Nhot_Labels_TopN\"] = train_df[\"attribute_ids_TopN\"].apply(lambda x: Nhot_encoding(x, take_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(823)\n",
    "np.random.seed(823)\n",
    "\n",
    "ds_allTrain = iMetsDataset(train_df,load_path+'train',\n",
    "                            transform = transforms.Compose([\n",
    "                            transforms.Resize((224,224)),\n",
    "                            transforms.ToTensor(),\n",
    "                            ]),\n",
    "                        )\n",
    "\n",
    "ds_train, ds_valid = data.random_split(ds_allTrain, [90000, 19237])\n",
    "\n",
    "ds_test = iMetsDataset(submit_df,load_path+'test',\n",
    "                            transform = transforms.Compose([\n",
    "                            transforms.Resize((224,224)),\n",
    "                            transforms.ToTensor(),\n",
    "                            ]),\n",
    "                           mode='test'\n",
    "                        )\n",
    "\n",
    "if type(ds_train.indices) == torch.Tensor:\n",
    "    ds_train.indices = ds_train.indices.numpy()\n",
    "    ds_valid.indices = ds_valid.indices.numpy()\n",
    "\n",
    "\n",
    "dataloader_train = data.DataLoader(dataset=ds_train,batch_size=batch_size,shuffle=True)\n",
    "dataloader_valid = data.DataLoader(dataset=ds_valid,batch_size=batch_size,shuffle=False)\n",
    "dataloader_test = data.DataLoader(dataset=ds_test,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /home/sano/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
      "100%|██████████| 241530880/241530880 [00:02<00:00, 112186496.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(823)\n",
    "np.random.seed(823)\n",
    "\n",
    "\n",
    "resnet152 = models.resnet152(pretrained=True)\n",
    "\n",
    "# Newly created modules have require_grad=True by default\n",
    "num_features = resnet152.fc.in_features\n",
    "features = list(resnet152.fc.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, take_num)]) # Add our layer\n",
    "resnet152.fc = nn.Sequential(*features) # Replace the model classifier\n",
    "# load weight\n",
    "# resnet152.load_state_dict(torch.load('model_weight/resnet152/resnet152_FocalLoss_epoch7.pkl'))\n",
    "model = resnet152.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = FocalLoss(gamma=2, logits=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train,eval,predictの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "def train(epoch, writer):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    steps = len(ds_train)//batch_size\n",
    "    for step, (images, labels) in enumerate(dataloader_train, 1):\n",
    "        global global_step\n",
    "        global_step += 1\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 150 == 0:\n",
    "            elapsed_time = time.time() - start\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.10f, time: %d分%d秒' % (epoch, epochs, step, steps, loss.item(), elapsed_time//60, int(elapsed_time % 60)))\n",
    "            writer.add_scalar('train/train_loss', loss.item() , global_step)\n",
    "\n",
    "            \n",
    "def eval(epoch, writer):\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    fbeta_lst = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader_valid):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            outputs_topN = np.argsort(outputs, axis=1)[:,-extract_attribute:]\n",
    "            outputs_topN_Nhots = Nhot_encoding(outputs_topN, take_num)\n",
    "            fbeta_lst.append(fbeta_score(labels,outputs_topN_Nhots, beta=2 ,average='samples'))\n",
    "            print('A')\n",
    "    elapsed_time = time.time() - start\n",
    "    print(\"Val Acc : %.10f, time: %d分%d秒\" % (sum(fbeta_lst)/len(fbeta_lst), elapsed_time//60, int(elapsed_time % 60)))\n",
    "    writer.add_scalar('eval/val_acc', sum(fbeta_lst)*100/len(fbeta_lst), epoch)\n",
    "    \n",
    "\n",
    "def predict():\n",
    "    pred_attr = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, images in enumerate(dataloader_test,1):\n",
    "            images = images.to(device)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            outputs_topN = np.argsort(outputs, axis=1)[:,-extract_attribute:]\n",
    "            for attr in outputs_topN:\n",
    "                pred_attr.append(attr)\n",
    "            if i % 10 == 0:\n",
    "                sys.stdout.write('\\r[%d/%d]' % (min((i * batch_size),test_size), test_size))\n",
    "                sys.stdout.flush()\n",
    "    return pred_attr          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [150/9000], Loss: 0.0036927534, time: 1分25秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-2b4bc684397c>\", line 5, in <module>\n",
      "    train(epoch, writer)\n",
      "  File \"<ipython-input-10-89709d4d68bb>\", line 6, in train\n",
      "    for step, (images, labels) in enumerate(dataloader_train, 1):\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in __next__\n",
      "    batch = self.collate_fn([self.dataset[i] for i in indices])\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 560, in <listcomp>\n",
      "    batch = self.collate_fn([self.dataset[i] for i in indices])\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataset.py\", line 107, in __getitem__\n",
      "    return self.dataset[self.indices[idx]]\n",
      "  File \"<ipython-input-4-0b98dd3ef58c>\", line 24, in __getitem__\n",
      "    image = self.transform(image)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 195, in __call__\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 246, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/PIL/Image.py\", line 1817, in resize\n",
      "    self.load()\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/PIL/ImageFile.py\", line 224, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/PIL/PngImagePlugin.py\", line 668, in load_read\n",
      "    return self.fp.read(read_bytes)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/posixpath.py\", line 443, in _joinrealpath\n",
      "    path, ok = _joinrealpath(path, os.readlink(newpath), seen)\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/sano/.pyenv/versions/anaconda3-5.3.1/envs/pytorch/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "writer = SummaryWriter(log_dir)\n",
    " \n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch, writer)\n",
    "    eval(epoch, writer)\n",
    "    torch.save(model.state_dict(), weight_path + '_epoch' + str(epoch)+'.pkl')\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = predict()\n",
    "# pred_str = list()\n",
    "# for lst in pred:\n",
    "#     pred_str.append(\" \".join(list(map(str, lst))))\n",
    "\n",
    "# submit_df.index = submit_df.index.map(lambda x:x.rstrip(\".png\"))\n",
    "# submit_df.attribute_ids = pred_str\n",
    "\n",
    "# submit_df.to_csv(\"submission.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
