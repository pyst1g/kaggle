{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fl: flip <br>\n",
    "ro: rotation <br>\n",
    "no: normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "log_dir =  runs/a\n",
      "weight save path =  ./model_weight//a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa7e49e75b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import models\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "# use_model = 'resnet152'\n",
    "this_file = ''\n",
    "\n",
    "if this_file == '': this_file = 'a'\n",
    "\n",
    "# data_path = \"~/Datasets/iMet_Colelction_2019\"\n",
    "# load_path = \"../input/\"\n",
    "load_path = \"/home/sano/Datasets/iMet_Colelction_2019/input/\"\n",
    "\n",
    "log_dir = 'runs/' + this_file\n",
    "weight_path = './model_weight/' + '/' + this_file\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)\n",
    "\n",
    "print('log_dir = ', log_dir)\n",
    "print('weight save path = ', weight_path)\n",
    "# 複数GPU使用宣言\n",
    "# if device == 'cuda:1':\n",
    "#     net = torch.nn.DataParallel(net) # make parallel\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "\n",
    "torch.manual_seed(823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_classes = 1103\n",
    "epochs = 50\n",
    "extract_attribute = 5 # 予測した上位何個を属性として出力するか\n",
    "num_workers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iMetsDataset(data.Dataset):\n",
    " \n",
    "    def __init__(self, df, root_dir, transform=None, mode='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (dataframe): ファイル名がindex、Nhot_LabelsカラムにNhot化したラベルを格納したDataframe\n",
    "            root_dir (string): 対象の画像ファイルが入っているフォルダ\n",
    "            transform (callable, optional): 施す変換\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "#         if type(idx) == torch.Tensor:\n",
    "#             idx = idx.item()\n",
    "        img_name = os.path.join(self.root_dir, self.df.index[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            label = self.df.iloc[idx].Nhot_Labels.astype('float32')\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "def Nhot_encoding(arr, l):\n",
    "    \"\"\"\n",
    "    Nhotエンコーディングを行う\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : ndarray\n",
    "        ラベル\n",
    "    l : int\n",
    "        総ラベル数\n",
    "    \"\"\"\n",
    "    if arr.ndim == 1:\n",
    "        ret = np.zeros(l,dtype='int')\n",
    "        ret[arr] = 1\n",
    "        return ret\n",
    "    else:\n",
    "        lst = list()\n",
    "        for i,_ in enumerate(arr):\n",
    "            lst.extend([i] * arr.shape[1])\n",
    "            \n",
    "        ret = np.zeros((arr.shape[0],l),dtype='int')\n",
    "        ret[lst,arr.flatten()] = 1\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = pd.read_csv(load_path + 'labels.csv')\n",
    "label_name = label_name.set_index(\"attribute_id\")\n",
    "submit_df = pd.read_csv(load_path + 'sample_submission.csv')\n",
    "submit_df[\"id\"] = submit_df[\"id\"].apply(lambda x: x + \".png\")\n",
    "submit_df = submit_df.set_index('id')\n",
    "test_size = len(submit_df)\n",
    "\n",
    "train_df = pd.read_csv(load_path + 'train.csv')\n",
    "train_size = len(train_df)\n",
    "train_df[\"attribute_ids\"] = train_df[\"attribute_ids\"].apply(lambda x: np.array([int(s) for s in x.split(\" \")]))\n",
    "train_df[\"Nhot_Labels\"] = train_df[\"attribute_ids\"].apply(lambda x: Nhot_encoding(x,1103))\n",
    "train_df[\"id\"] = train_df[\"id\"].apply(lambda x: x + \".png\")\n",
    "train_df = train_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(823)\n",
    "np.random.seed(823)\n",
    "\n",
    "ds_allTrain = iMetsDataset(train_df,load_path+'train',\n",
    "                            transform = transforms.Compose([\n",
    "                                transforms.Resize((384,384)),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomRotation((-20,20)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    [0.485, 0.456, 0.406], \n",
    "                                    [0.229, 0.224, 0.225]\n",
    "                                ),\n",
    "                            ]),\n",
    "                           mode ='train'\n",
    "                        )\n",
    "\n",
    "\n",
    "ds_train, ds_valid = data.random_split(ds_allTrain, [90000, 19237])\n",
    "ds_train.dataset = deepcopy(ds_allTrain)\n",
    "ds_valid.dataset.transform =  transform = transforms.Compose([\n",
    "                                transforms.Resize((384,384)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    [0.485, 0.456, 0.406], \n",
    "                                    [0.229, 0.224, 0.225]\n",
    "                                ),\n",
    "                            ])\n",
    "ds_test = iMetsDataset(submit_df,load_path+'test',\n",
    "                            transform = transforms.Compose([\n",
    "                                transforms.Resize((384,384)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    [0.485, 0.456, 0.406], \n",
    "                                    [0.229, 0.224, 0.225]\n",
    "                                ),\n",
    "                            ]),\n",
    "                           mode='test'\n",
    "                        )\n",
    "\n",
    "if type(ds_train.indices) == torch.Tensor:\n",
    "    ds_train.indices = ds_train.indices.numpy()\n",
    "    ds_valid.indices = ds_valid.indices.numpy()\n",
    "\n",
    "\n",
    "dataloader_train = data.DataLoader(dataset=ds_train,batch_size=batch_size,shuffle=True, num_workers=num_workers)\n",
    "dataloader_valid = data.DataLoader(dataset=ds_valid,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n",
    "dataloader_test = data.DataLoader(dataset=ds_test,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(823)\n",
    "np.random.seed(823)\n",
    "\n",
    "\n",
    "resnet152 = models.resnet152(pretrained=True)\n",
    "\n",
    "# Newly created modules have require_grad=True by default\n",
    "num_features = resnet152.fc.in_features\n",
    "features = list(resnet152.fc.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, num_classes)]) # Add our layer\n",
    "resnet152.fc = nn.Sequential(*features) # Replace the model classifier\n",
    "# load weight\n",
    "resnet152.load_state_dict(torch.load('model_weight/ResNet152_Focal_Loss_384_flrono_2_epoch13.pkl'))\n",
    "model = resnet152.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = FocalLoss(gamma=2, logits=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train,eval,predictの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "def train(epoch, writer):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    steps = len(ds_train)//batch_size\n",
    "    for step, (images, labels) in enumerate(dataloader_train, 1):\n",
    "        global global_step\n",
    "        global_step += 1\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 150 == 0:\n",
    "            elapsed_time = time.time() - start\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.10f, time: %d分%d秒' % (epoch, epochs, step, steps, loss.item(), elapsed_time//60, int(elapsed_time % 60)))\n",
    "            writer.add_scalar('train/train_loss', loss.item() , global_step)\n",
    "\n",
    "            \n",
    "def eval(epoch, writer):\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    fbeta_lst = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader_valid):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            outputs_topN = np.argsort(outputs, axis=1)[:,-extract_attribute:]\n",
    "            outputs_topN_Nhots = Nhot_encoding(outputs_topN, num_classes)\n",
    "            fbeta_lst.append(fbeta_score(labels,outputs_topN_Nhots, beta=2 ,average='samples'))\n",
    "    elapsed_time = time.time() - start\n",
    "    print(\"Val Acc : %.10f, time: %d分%d秒\" % (sum(fbeta_lst)/len(fbeta_lst), elapsed_time//60, int(elapsed_time % 60)))\n",
    "    writer.add_scalar('eval/val_acc', sum(fbeta_lst)*100/len(fbeta_lst), epoch)\n",
    "    \n",
    "\n",
    "def predict(thr_attr):\n",
    "    pred_attr = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, images in enumerate(dataloader_test,1):\n",
    "            images = images.to(device)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            outputs_label = np.zeros_like(outputs)\n",
    "            for j in range(num_classes):\n",
    "                outputs_label[:,j] = outputs[:,j] >= thr_attr[j]\n",
    "#             pos, label = np.where(outputs >= thr_attr)\n",
    "            pos, label = np.where(outputs_label)\n",
    "            for j in range(len(outputs)):\n",
    "                pred_attr.append(label[pos == j])\n",
    "                \n",
    "#             outputs_topN = np.argsort(outputs, axis=1)[:,-extract_attribute:]\n",
    "#             for attr in outputs_arg:\n",
    "#                 pred_attr.append(attr)\n",
    "            if i % 10 == 0:\n",
    "                sys.stdout.write('\\r[%d/%d]' % (min((i * batch_size),test_size), test_size))\n",
    "                sys.stdout.flush()\n",
    "    return pred_attr          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pred_prop():\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    steps = len(ds_valid)\n",
    "    propotion_arr = list()\n",
    "    labels_arr = list()\n",
    "\n",
    "    # ラベル確率を推論\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader_valid,1):\n",
    "            images = images.to(device)\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            labels_arr.extend(labels)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            propotion_arr.extend(outputs)\n",
    "        #         outputs_topN = np.argsort(outputs, axis=1)[:,-extract_attribute:]\n",
    "        #         for attr in outputs_topN:\n",
    "        #             pred_attr.append(attr)\n",
    "            if i % 10 == 0:\n",
    "                elapsed_time = time.time() - start\n",
    "                print('\\r[%d/%d], time: %d分%d秒' % (min((i * batch_size),steps), steps, elapsed_time//60, int(elapsed_time % 60)))\n",
    "                clear_output(wait=True)\n",
    "\n",
    "\n",
    "    propotion_arr = np.asarray(propotion_arr)\n",
    "    labels_arr = np.asarray(labels_arr)\n",
    "    return propotion_arr, labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[17400/19237], time: 3分7秒\n"
     ]
    }
   ],
   "source": [
    "propotion_arr, labels_arr = _pred_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propotion_arr_train, propotion_arr_val, labels_arr_train, labels_arr_val  = train_test_split(propotion_arr,  labels_arr, test_size=0.4, random_state = 823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_only_threthold(propotion_arr, labels_arr, sample_num = 100000):\n",
    "    start = time.time()\n",
    "    \n",
    "#     model.eval()\n",
    "#     steps = len(ds_valid)\n",
    "#     propotion_arr = list()\n",
    "#     labels_arr = list()\n",
    "\n",
    "#     # ラベル確率を推論\n",
    "#     with torch.no_grad():\n",
    "#         for i, (images, labels) in enumerate(dataloader_valid,1):\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.cpu().detach().numpy()\n",
    "#             labels_arr.extend(labels)\n",
    "#             outputs = torch.sigmoid(model(images))\n",
    "#             outputs = outputs.cpu().detach().numpy()\n",
    "#             propotion_arr.extend(outputs)\n",
    "#         #         outputs_topN = np.argsort(outputs, axis=1)[:,-extract_attribute:]\n",
    "#         #         for attr in outputs_topN:\n",
    "#         #             pred_attr.append(attr)\n",
    "#             if i % 10 == 0:\n",
    "#                 elapsed_time = time.time() - start\n",
    "#                 print('\\r[%d/%d], time: %d分%d秒' % (min((i * batch_size),steps), steps, elapsed_time//60, int(elapsed_time % 60)))\n",
    "#                 clear_output(wait=True)\n",
    "\n",
    "\n",
    "#     propotion_arr = np.asarray(propotion_arr)\n",
    "#     labels_arr = np.asarray(labels_arr)\n",
    "\n",
    "    pc = deepcopy(propotion_arr)\n",
    "    lc = deepcopy(labels_arr)\n",
    "    pc = np.reshape(pc,-1)\n",
    "    lc = np.reshape(lc,-1)\n",
    "    idx = np.argsort(pc)\n",
    "    pc = pc[idx]\n",
    "    lc = lc[idx]\n",
    "\n",
    "    TP = np.sum(labels_arr==1, axis=1)\n",
    "    FN = np.zeros_like(TP)\n",
    "    FP = np.sum(labels_arr==0, axis=1)\n",
    "    TN = np.zeros_like(TP)\n",
    "\n",
    "    f2 = np.zeros_like(TP)\n",
    "\n",
    "    tmp_max = 0\n",
    "    max_thr = 0\n",
    "    pos = 0\n",
    "    for i, thr in enumerate(np.logspace(-4,0,sample_num)):\n",
    "        if i % 100 == 0:\n",
    "            elapsed_time = time.time() - start\n",
    "            print('\\r[%d/%d], time: %d分%d秒' % (i, sample_num, elapsed_time//60, int(elapsed_time % 60)))\n",
    "            clear_output(wait=True)\n",
    "        while pos < len(pc) and pc[pos] < thr:\n",
    "            if lc[pos] == 0:\n",
    "                FP[idx[pos] // num_classes] -= 1\n",
    "                TN[idx[pos] // num_classes] += 1\n",
    "            else:\n",
    "                TP[idx[pos] // num_classes] -= 1\n",
    "                FN[idx[pos] // num_classes] += 1\n",
    "#             if pos % 100000 == 0: print(pos)\n",
    "            pos += 1\n",
    "\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f2_arr = 5*(precision * recall) / (4*precision + recall)\n",
    "        f2_arr[np.isnan(f2_arr)] = 0\n",
    "        f2 = np.mean(f2_arr)\n",
    "        if f2 > tmp_max:\n",
    "            tmp_max = f2\n",
    "            max_thr = thr\n",
    "    return max_thr, tmp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thr, f2_train = make_only_threthold(propotion_arr_train, labels_arr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = deepcopy(propotion_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_attr = np.ones(num_classes) * thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上位何個取るか\n",
    "take_num = 1103\n",
    "\n",
    "cnt_attribute = Counter()\n",
    "for i in train_df.attribute_ids:\n",
    "    cnt_attribute.update(i)\n",
    "\n",
    "freq_attr = np.asarray(cnt_attribute.most_common(take_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "pred_labels_train = propotion_arr_train >= thr\n",
    "TP_train = ((pred_labels_train == 1) * (labels_arr_train == 1)).sum(axis=1)\n",
    "FP_train = ((pred_labels_train == 1) * (labels_arr_train == 0)).sum(axis=1)\n",
    "FN_train = ((pred_labels_train == 0) * (labels_arr_train == 1)).sum(axis=1)\n",
    "TN_train = ((pred_labels_train == 0) * (labels_arr_train == 0)).sum(axis=1)\n",
    "\n",
    "# valid\n",
    "pred_labels_val = propotion_arr_val >= thr\n",
    "TP_val = ((pred_labels_val == 1) * (labels_arr_val == 1)).sum(axis=1)\n",
    "FP_val = ((pred_labels_val == 1) * (labels_arr_val == 0)).sum(axis=1)\n",
    "FN_val = ((pred_labels_val == 0) * (labels_arr_val == 1)).sum(axis=1)\n",
    "TN_val = ((pred_labels_val == 0) * (labels_arr_val == 0)).sum(axis=1)\n",
    "\n",
    "# each score\n",
    "precision = TP_val / (TP_val + FP_val)\n",
    "recall = TP_val / (TP_val + FN_val)\n",
    "f2_arr_val = 5*(precision * recall) / (4*precision + recall)\n",
    "f2_arr_val[np.isnan(f2_arr_val)] = 0\n",
    "f2_val = np.mean(f2_arr_val)\n",
    "# print(\"f2_train = \", f2_train)\n",
    "# print(\"f2_val = \", f2_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_num = 10000\n",
    "f2_arr_train = list()\n",
    "f2_arr_val = list()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for k, (attr, _) in enumerate(freq_attr):\n",
    "    idx = np.argsort(propotion_arr_train[:,attr])\n",
    "    tmp_max = 0\n",
    "    max_thr = 0\n",
    "    pos = 0\n",
    "\n",
    "    tmpTP = TP_train + (pred_labels_train[:,attr]==0) * (labels_arr_train[:,attr]==1)\n",
    "    tmpFP = FP_train + (pred_labels_train[:,attr]==0) * (labels_arr_train[:,attr]==0)\n",
    "    tmpFN = FN_train - (pred_labels_train[:,attr]==0) * (labels_arr_train[:,attr]==1)\n",
    "    tmpTN = TN_train - (pred_labels_train[:,attr]==0) * (labels_arr_train[:,attr]==0)\n",
    "\n",
    "    for i, t in enumerate(np.logspace(-3,0,sample_num)):\n",
    "#         if i % 100 == 0:\n",
    "#             elapsed_time = time.time() - start\n",
    "#             print('\\r[%d/%d], time: %d分%d秒' % (i, sample_num, elapsed_time//60, int(elapsed_time % 60)))\n",
    "#             clear_output(wait=True)\n",
    "    #     print(pos,i)\n",
    "        while pos < len(propotion_arr_train[:,attr]) and propotion_arr_train[idx[pos],attr] < t:\n",
    "            if labels_arr_train[idx[pos],attr] == 0:\n",
    "                tmpFP[idx[pos]] -= 1\n",
    "                tmpTN[idx[pos]] += 1\n",
    "            else:\n",
    "                tmpTP[idx[pos]] -= 1\n",
    "                tmpFN[idx[pos]] += 1\n",
    "            pos += 1\n",
    "\n",
    "        precision = tmpTP / (tmpTP + tmpFP)\n",
    "        recall = tmpTP / (tmpTP + tmpFN)\n",
    "        f2_arr = 5*(precision * recall) / (4*precision + recall)\n",
    "        f2_arr[np.isnan(f2_arr)] = 0\n",
    "        f2 = np.mean(f2_arr)\n",
    "        if f2 > tmp_max:\n",
    "            tmp_max = f2\n",
    "            max_thr = t\n",
    "\n",
    "    # train\n",
    "    thr_attr[attr] = max_thr\n",
    "    pred_labels_train[:,attr] = (propotion_arr_train[:,attr] >= max_thr)\n",
    "    TP_train = ((pred_labels_train == 1) * (labels_arr_train == 1)).sum(axis=1)\n",
    "    FP_train = ((pred_labels_train == 1) * (labels_arr_train == 0)).sum(axis=1)\n",
    "    FN_train = ((pred_labels_train == 0) * (labels_arr_train == 1)).sum(axis=1)\n",
    "    TN_train = ((pred_labels_train == 0) * (labels_arr_train == 0)).sum(axis=1)\n",
    "    \n",
    "    precision = TP_train / (TP_train + FP_train)\n",
    "    recall = TP_train / (TP_train + FN_train)\n",
    "    f2_arr = 5*(precision * recall) / (4*precision + recall)\n",
    "    f2_arr[np.isnan(f2_arr)] = 0\n",
    "    f2_arr_train.append(np.mean(f2_arr))\n",
    "    \n",
    "    # valid\n",
    "    pred_labels_val[:,attr] = (propotion_arr_val[:,attr] >= max_thr)\n",
    "    TP_val = ((pred_labels_val == 1) * (labels_arr_val == 1)).sum(axis=1)\n",
    "    FP_val = ((pred_labels_val == 1) * (labels_arr_val == 0)).sum(axis=1)\n",
    "    FN_val = ((pred_labels_val == 0) * (labels_arr_val == 1)).sum(axis=1)\n",
    "    TN_val = ((pred_labels_val == 0) * (labels_arr_val == 0)).sum(axis=1)\n",
    "    \n",
    "    precision = TP_val / (TP_val + FP_val)\n",
    "    recall = TP_val / (TP_val + FN_val)\n",
    "    f2_arr = 5*(precision * recall) / (4*precision + recall)\n",
    "    f2_arr[np.isnan(f2_arr)] = 0\n",
    "    f2_arr_val.append(np.mean(f2_arr))\n",
    "    print(\"{}　train_score = {:.10f}, valid_score = {:.10f}\".format(k,f2_arr_train[-1],f2_arr_val[-1]))\n",
    "#     print(\"valid_score = {}\".format(f2_arr_val[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_thr = deepcopy(thr_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(thr_attr)\n",
    "pred_str = list()\n",
    "for lst in pred:\n",
    "    pred_str.append(\" \".join(list(map(str, lst))))\n",
    "\n",
    "submit_df.index = submit_df.index.map(lambda x:x.rstrip(\".png\"))\n",
    "submit_df.attribute_ids = pred_str\n",
    "\n",
    "submit_df.to_csv(\"submission.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
